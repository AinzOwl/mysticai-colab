{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOdFVdGjRGAupXD2XYA3LWC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AinzOwl/mysticai-colab/blob/main/mystic-ai%20deploy%20llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers einops pipeline-ai typing"
      ],
      "metadata": {
        "id": "lHWixiUpFOa8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers einops typing pipeline-ai"
      ],
      "metadata": {
        "id": "jBqVztp5PefZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mystic_input = input(\"enter mystic api: \")\n",
        "!pipeline cluster login catalyst-api {mystic_input} -u https://mystic.ai -a"
      ],
      "metadata": {
        "id": "ftOVHA-YWsTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "\n",
        "from pipeline import Pipeline, Variable, entity, pipe\n",
        "from pipeline.cloud import compute_requirements, environments, pipelines\n",
        "from pipeline.objects.graph import InputField, InputSchema"
      ],
      "metadata": {
        "id": "R-6VEd6NNt-J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelKwargs(InputSchema):\n",
        "    system: str | None = InputField(\n",
        "        default = \"I am OrcaPhi. The following is my internal dialogue as an AI assistant.\\n\" \\\n",
        "            \"Today is September 15, 2023. I have no access to outside tools, news, or current events.\\n\" \\\n",
        "            \"I carefully provide accurate, factual, thoughtful, nuanced answers and am brilliant at reasoning.\\n\" \\\n",
        "            \"I think through my answers step-by-step to be sure I always get the right answer.\\n\" \\\n",
        "            \"I think more clearly if I write out my thought process in a scratchpad manner first; therefore, I always \" \\\n",
        "            \"explain background context, assumptions, and step-by-step thinking BEFORE trying to answer a question.\" \\\n",
        "            \"Take a deep breath and think calmly about everything presented.\",\n",
        "        title = \"System Prompt\",\n",
        "        description = \"Enter a description of your system\"\n",
        "    )\n",
        "    do_sample: bool | None = InputField(default=True)\n",
        "    use_cache: bool | None = InputField(default=True)\n",
        "    temperature: float | None = InputField(default=0.6)\n",
        "    repetition_penalty: float | None = InputField(default=1.1)\n",
        "    top_p: float | None = InputField(default=0.9)\n",
        "    max_length: int | None = InputField(default=100, ge=1, le=4096)\n",
        "    presence_penalty: float | None = InputField(default=1.0)"
      ],
      "metadata": {
        "id": "JaffuDQfNvjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@entity\n",
        "class LlamaPipeline:\n",
        "    def __init__(self) -> None:\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "        self.streamer = None\n",
        "\n",
        "    @pipe(on_startup=True, run_once=True)\n",
        "    def load_model(self) -> None:\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\"Open-Orca/oo-phi-1_5\", trust_remote_code=True, torch_dtype=torch.bfloat16).to(\"cuda\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(\"Open-Orca/oo-phi-1_5\", trust_remote_code=True, torch_dtype=torch.bfloat16)\n",
        "\n",
        "\n",
        "    @pipe\n",
        "    def inference(self, prompt: str, kwargs: ModelKwargs) -> List[str]:\n",
        "\n",
        "\n",
        "        prefix = \"<|im_start|>\"\n",
        "        suffix = \"<|im_end|>\\n\"\n",
        "        sys_format = prefix + \"system\\n\" + kwargs.system + suffix\n",
        "        user_format = prefix + \"user\\n\" + prompt + suffix\n",
        "        assistant_format = prefix + \"assistant\\n\"\n",
        "        input_text = sys_format + user_format + assistant_format\n",
        "\n",
        "        generation_config = GenerationConfig(\n",
        "            max_length=kwargs.max_length, temperature=kwargs.temperature, top_p=kwargs.top_p, repetition_penalty=kwargs.repetition_penalty,\n",
        "            do_sample=kwargs.do_sample, use_cache=kwargs.use_cache,\n",
        "            eos_token_id=self.tokenizer.eos_token_id, pad_token_id=self.tokenizer.pad_token_id,\n",
        "            transformers_version=\"4.33.1\"\n",
        "            )\n",
        "\n",
        "        inputs = self.tokenizer(input_text, return_tensors=\"pt\", return_attention_mask=False).to('cuda')\n",
        "        outputs = self.model.generate(**inputs, generation_config=generation_config)\n",
        "\n",
        "        return self.tokenizer.batch_decode(outputs)[0]\n"
      ],
      "metadata": {
        "id": "vo_TjwZ8Kdcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with Pipeline() as builder:\n",
        "    prompt = Variable(str)\n",
        "    kwargs = Variable(ModelKwargs)\n",
        "\n",
        "    _pipeline = LlamaPipeline()\n",
        "    _pipeline.load_model()\n",
        "    out = _pipeline.inference(prompt, kwargs)\n",
        "\n",
        "    builder.output(out)\n",
        "\n",
        "\n",
        "my_pipeline = builder.get_pipeline()\n",
        "\n",
        "\n",
        "try:\n",
        "    environments.create_environment(\n",
        "        \"oophi1_5\",\n",
        "        python_requirements=[\n",
        "            \"torch==2.0.1\",\n",
        "            \"transformers==4.33.1\",\n",
        "            \"einops==0.6.1\"\n",
        "        ],\n",
        "    )\n",
        "except Exception:\n",
        "    pass"
      ],
      "metadata": {
        "id": "haoerkrsN02i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = my_pipeline.run(\n",
        "    \"Tell me a short story about an orca swimming in the sea\",\n",
        "    ModelKwargs(),\n",
        ")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "XegkuxiCVL6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pipelines.upload_pipeline(\n",
        "    my_pipeline,\n",
        "    \"Ainzoil/oo_phi1_5:latest\",\n",
        "    environment_id_or_name=\"oophi1_5\",\n",
        "    required_gpu_vram_mb=20_000,\n",
        "    accelerators=[\n",
        "        compute_requirements.Accelerator.nvidia_a100,\n",
        "    ],\n",
        ")\n"
      ],
      "metadata": {
        "id": "sJh_2RjUN2Gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = my_pipeline.run(\n",
        "    \"Hello, how are you?\",\n",
        "    ModelKwargs(),\n",
        ")\n",
        "\n",
        "print(output)"
      ],
      "metadata": {
        "id": "FPS-mkTgTXkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Pipeline ID: {result.id}\")\n",
        "output = my_pipeline.run(\n",
        "    \"Hello, how are you?\",\n",
        "    ModelKwargs(),\n",
        ")\n",
        "\n",
        "\n",
        "# print(output)"
      ],
      "metadata": {
        "id": "8c9AVIlQN8FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import webbrowser\n",
        "\n",
        "from pipeline.cloud.pipelines import run_pipeline\n",
        "\n",
        "output = run_pipeline(\n",
        "    # Pipeline pointer or ID\n",
        "    \"stabilityai/stable-diffusion-xl-refiner-1.0:v1\",\n",
        "    # Prompt\n",
        "    \"Mountain winds and babbling springs and moonlight seas.\",\n",
        "    # Model kwargs\n",
        "    dict(\n",
        "        denoising_end=0.8,\n",
        "        num_inference_steps=25,\n",
        "    ),\n",
        ")\n",
        "\n",
        "result = output.result.result_array()\n",
        "\n",
        "# Extract the image URL from the result\n",
        "url = result[0][0][\"file\"][\"url\"]\n",
        "\n",
        "# Open the URL in the default web browser\n",
        "webbrowser.open(url)"
      ],
      "metadata": {
        "id": "lE0nFrPjPVwQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}